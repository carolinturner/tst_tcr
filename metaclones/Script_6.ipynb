{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from os.path import join, dirname, abspath\n",
    "import contextlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "import igraph\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "import pyrepseq as prs\n",
    "import pyrepseq.plotting as pp\n",
    "from metaclonotypist import *\n",
    "\n",
    "plt.style.use('seaborn-v0_8-paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change here to define MHC class and path to Gliph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhc_class = 'II'\n",
    "GLIPH2_PATH = '/home/innate2adaptive/gliph2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and run GLIPH2 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load TCRseq data\n",
    "chain = 'beta'\n",
    "chain_letter = chain[0].upper()\n",
    "\n",
    "df = pd.read_csv(f'data/combined_subsampled_5000_10000_{chain}.csv.gz')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna(subset=[f'TR{chain_letter}V', f'CDR3{chain_letter}'])\n",
    "df = df[df[f'CDR3{chain_letter}'].apply(len)>5]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HLA data\n",
    "hla = pd.read_csv('data/hladata.csv', index_col=0)\n",
    "hlas = flatten_hlas(hla)\n",
    "hlas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmethod = 'fisher'\n",
    "mincount = 2\n",
    "min_donors = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter clones < mincount\n",
    "df = df[df['clonal_count']>=mincount]\n",
    "# only keep samples found in both datasets\n",
    "print(set(df['UIN'])-set(hlas.index))\n",
    "df = df[df['UIN'].isin(hlas.index)]\n",
    "hlas = hlas.loc[list(set(df['UIN']))]\n",
    "len(df['UIN'].unique()), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter hlas < min_donors\n",
    "hlas = hlas[hlas.columns[hlas.sum(axis=0)>=min_donors]]\n",
    "# filter MHC class\n",
    "if mhc_class == 'both':\n",
    "    pass\n",
    "elif mhc_class == 'I':\n",
    "    hlas = hlas[hlas.columns[~hlas.columns.str.startswith('D')]]\n",
    "elif mhc_class == 'II':\n",
    "    hlas = hlas[hlas.columns[hlas.columns.str.startswith('D')]]\n",
    "else:\n",
    "    raise NotImplementedError(\"mhc_class needs to be in ['both', 'I', 'II']\")\n",
    "len(hlas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat hla data frame to match Gliph requirements\n",
    "hla['#subject'] = hla.index\n",
    "hla = hla[ ['#subject'] + [ col for col in hla.columns if col != '#subject' ] ]\n",
    "\n",
    "hla.to_csv('hla_file.txt', index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path import Path\n",
    "\n",
    "def GLIPH2(data,\n",
    "           global_convergence_cutoff=1,\n",
    "           all_aa_interchangeable=1,\n",
    "           local_min_pvalue=0.001,\n",
    "           outfile=None):\n",
    "    \n",
    "    with Path(GLIPH2_PATH):\n",
    "    \n",
    "        data = data.copy()\n",
    "        data['CDR3a'] = 'NA'\n",
    "        data['subject:condition'] = 'NA'\n",
    "        data = data[['CDR3B', 'TRBV',\n",
    "                     'TRBJ', 'CDR3a',\n",
    "                     'UIN', 'clonal_count']]\n",
    "\n",
    "        data.to_csv('metarepertoire.txt', index=False, header=False, sep='\\t')\n",
    "\n",
    "        print('Clustering {} sequences with GLIPH2.'.format(len(data)))\n",
    "\n",
    "        parameters = \\\n",
    "    f\"\"\"# Ignored line\n",
    "    out_prefix=gliph2_beta_mhc{mhc_class}\n",
    "    cdr3_file=metarepertoire.txt\n",
    "    hla_file=hla_file.txt\n",
    "    refer_file=ref_CD48_v2.0.fa\n",
    "    v_usage_freq_file=ref_V_CD48_v2.0.txt\n",
    "    cdr3_length_freq_file=ref_L_CD48_v2.0.txt\n",
    "    local_min_pvalue={local_min_pvalue}\n",
    "    p_depth = 1000\n",
    "    global_convergence_cutoff = {global_convergence_cutoff}\n",
    "    simulation_depth=1000\n",
    "    kmer_min_depth=3\n",
    "    local_min_OVE=10\n",
    "    algorithm=GLIPH2\n",
    "    all_aa_interchangeable={all_aa_interchangeable}\n",
    "    \"\"\"\n",
    "        with open('parameters_custom', 'w') as f:\n",
    "            f.write(parameters)\n",
    "\n",
    "        # Perform gliph2 algorithm on test sequences\n",
    "        t0 = time.time()\n",
    "        os.system('./irtools.centos -c parameters_custom')\n",
    "        t1 = time.time()\n",
    "        t = t1 - t0\n",
    "\n",
    "        print('Elapsed time: {} seconds.'.format(t))\n",
    "\n",
    "        # Reformat gliph2 clustering results\n",
    "        clusters = []\n",
    "        # nodelist = {'CDR3': [], 'cluster': []}\n",
    "        with open(f'gliph2_beta_mhc{mhc_class}_cluster.txt', 'r') as f:\n",
    "            results = f.read().splitlines()\n",
    "        c = 0\n",
    "        for line in results:\n",
    "            columns = line.split(' ')\n",
    "            motif = columns[3]\n",
    "            cluster = columns[4:]\n",
    "            if len(cluster) >= 2:\n",
    "                nodes = pd.DataFrame({'junction_aa': cluster})\n",
    "                nodes['cluster'] = c\n",
    "                nodes['motif'] = motif\n",
    "                clusters.append(nodes)\n",
    "                c += 1\n",
    "\n",
    "        if outfile:\n",
    "            print('Saving output to: \\n --> {}'.format(outfile))\n",
    "            nodelist.to_csv(outfile, sep='\\t', index=False)\n",
    "\n",
    "    clusters = pd.concat(clusters)\n",
    "    exclusive = clusters.drop_duplicates('junction_aa', keep='first')\n",
    "    return exclusive, t, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclusive, t, clusters = GLIPH2(df, all_aa_interchangeable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform HLA association analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = pd.read_csv(f'{GLIPH2_PATH}/gliph2_beta_mhc{mhc_class}_cluster.csv')\n",
    "clusters = clusters[clusters['number_subject']>=min_donors]\n",
    "len(clusters['index'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_rn = clusters.rename(columns=dict(index='cluster', Sample='Sample.ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_association = hla_association(clusters_rn, hlas,\n",
    "                                      method=testmethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_association.to_csv(f'{GLIPH2_PATH}/gliph2_beta_mhc{mhc_class}_clusterassociation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmetaclones = len(cluster_association[cluster_association['significant']]['cluster'].unique())\n",
    "cluster_association['significant'].sum(), nmetaclones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_association_noinf = cluster_association.replace(np.inf, 400, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
    "sns.scatterplot(ax=ax, data=cluster_association_noinf,\n",
    "                x='odds_ratio',\n",
    "                y=-np.log10(cluster_association_noinf['pvalue']),\n",
    "                hue='significant',\n",
    "                s=5)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('p value')\n",
    "ax.set_xlabel('odds ratio')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_metaclones = cluster_association[cluster_association['significant']]\n",
    "hla_metaclones.to_csv(f'{GLIPH2_PATH}/gliph2_beta_mhc{mhc_class}_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain only most significant HLA association per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_metaclones_unique = hla_metaclones.sort_values('pvalue'\n",
    "                            ).drop_duplicates(subset='cluster', keep='first'\n",
    "                            ).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add clustered CDR3 amino acid sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_metaclones_unique['CDR3s'] = hla_metaclones_unique['cluster'].apply(lambda x:\n",
    "                                    '|'.join(clusters_rn[(clusters_rn['cluster']==x)][f'TcRb']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_metaclones_unique = hla_metaclones_unique.merge(clusters_rn[['cluster', 'pattern']], on='cluster', how='left').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_metaclones_unique.to_csv(f'{GLIPH2_PATH}/gliph2_beta_mhc{mhc_class}_output2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
